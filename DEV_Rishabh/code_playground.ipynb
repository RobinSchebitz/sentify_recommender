{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Spotify Playlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and setting up authentication with Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data from Spotify dataset\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "# Replace 'your_client_id', 'your_client_secret', and 'your_redirect_uri' with your actual values\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(\n",
    "    client_id='5093ad0ad45f435ca7ce43b2afd406b9',\n",
    "    client_secret='f151962f414144aa85494be81ccd8e20',\n",
    "    redirect_uri='http://localhost:8888/callback',\n",
    "    scope=\"user-library-read playlist-read-private\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching data from Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''playlist_id = '6JOysTE0drK9yDi1Xs4FKy'  # Replace with the actual playlist ID\n",
    "playlist = sp.playlist(playlist_id)\n",
    "print(playlist['name'])\n",
    "for item in playlist['tracks']['items']:\n",
    "    track = item['track']\n",
    "    print(track['name'], '-', track['artists'][0]['name'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch saved tracks\n",
    "playlist_id = '6JOysTE0drK9yDi1Xs4FKy'\n",
    "\n",
    "results = sp.playlist_tracks(playlist_id)\n",
    "tracks_data = {\n",
    "    'Name': [],\n",
    "    'Artist': [],\n",
    "    'Album': [],\n",
    "    'Release Date': []\n",
    "}\n",
    "\n",
    "for item in results['items']:\n",
    "    track = item.get('track')\n",
    "    if track:  # Check if track details are present\n",
    "        name = track.get('name', 'No Title Available')\n",
    "        artist_name = track['artists'][0].get('name', 'No Artist Available') if track['artists'] else 'No Artists'\n",
    "        album_name = track['album'].get('name', 'No Album Available') if track['album'] else 'No Album'\n",
    "        release_date = track['album'].get('release_date', 'No Release Date Available') if track['album'] else 'No Release Info'\n",
    "        \n",
    "        tracks_data['Name'].append(name)\n",
    "        tracks_data['Artist'].append(artist_name)\n",
    "        tracks_data['Album'].append(album_name)\n",
    "        tracks_data['Release Date'].append(release_date)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tracks = pd.DataFrame(tracks_data)\n",
    "print(df_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "# Initialize the Spotify client\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(\n",
    "    client_id='your_client_id',\n",
    "    client_secret='your_client_secret',\n",
    "    redirect_uri='http://localhost:8888/callback',\n",
    "    scope='user-library-read'))\n",
    "\n",
    "# Search for tracks by genre\n",
    "genre = \"rock\"  # Example genre\n",
    "results = sp.search(q='genre:' + genre, type='track', limit=50)\n",
    "tracks_data = {\n",
    "    'Name': [],\n",
    "    'Artist': [],\n",
    "    'Genre': genre,\n",
    "    'Popularity': []\n",
    "}\n",
    "\n",
    "for track in results['tracks']['items']:\n",
    "    tracks_data['Name'].append(track['name'])\n",
    "    tracks_data['Artist'].append(track['artists'][0]['name'])\n",
    "    tracks_data['Popularity'].append(track['popularity'])\n",
    "\n",
    "# Print or process the results\n",
    "print([tracks_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Song Data with Lyrics using Genius API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "# Initialize Genius API\n",
    "genius = lyricsgenius.Genius('put_your_token_here')\n",
    "\n",
    "genius.remove_section_headers = True\n",
    "\n",
    "# Spotify Global Top 50 playlist ID\n",
    "playlist_id = '37i9dQZEVXbMDoHDwVN2tF'  # Example ID, update as necessary\n",
    "\n",
    "results = sp.playlist_tracks(playlist_id)\n",
    "tracks_data = []\n",
    "\n",
    "# Fetch track details from Spotify\n",
    "for item in results['items']:\n",
    "    track = item['track']\n",
    "    track_info = {\n",
    "        'Name': track['name'],\n",
    "        'Artist': track['artists'][0]['name'] if track['artists'] else 'Unknown',\n",
    "        'Album': track['album']['name'] if track['album'] else 'Unknown',\n",
    "        'Release Date': track['album']['release_date'] if track['album'] else 'Unknown',\n",
    "        'Popularity': track['popularity']\n",
    "    }\n",
    "    # Fetch lyrics using Genius\n",
    "    song = genius.search_song(track_info['Name'], track_info['Artist'])\n",
    "    track_info['Lyrics'] = song.lyrics if song else 'Lyrics not found'\n",
    "    \n",
    "    tracks_data.append(track_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df_tracks = pd.DataFrame(tracks_data)\n",
    "\n",
    "# Set column names and display the DataFrame\n",
    "df_tracks.columns = ['Track Name', 'Artist', 'Album', 'Release Date', 'Popularity', 'Lyrics']\n",
    "print(df_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "df_tracks.to_csv('spotify_songs_with_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "\n",
    "Clean the lyrics by removing punctuation, converting to lowercase, and other typical text cleaning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Removing punctuation\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the lyrics column\n",
    "df_tracks['cleaned_lyrics'] = df_tracks['Lyrics'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_tracks['cleaned_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment intensity analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the compound sentiment score\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']  # Return the compound score\n",
    "\n",
    "# Apply the sentiment analysis function to the cleaned lyrics\n",
    "df_tracks['sentiment_score'] = df_tracks['cleaned_lyrics'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Sentiment Labels\n",
    "\n",
    "Classifying the sentiments as positive, neutral or negative based on the computed score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sentiment(score):\n",
    "    if score > 0.05:\n",
    "        return 'positive'\n",
    "    elif score < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment label assignment\n",
    "df_tracks['sentiment_label'] = df_tracks['sentiment_score'].apply(assign_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tracks[['Track Name', 'Artist', 'sentiment_score', 'sentiment_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks[['Track Name', 'Artist', 'sentiment_score', 'sentiment_label']].to_csv('test_sentiment.csv', index=False)\n",
    "\n",
    "test_sentiment = pd.read_csv('test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Json file to Pandas readable format\n",
    "\n",
    "Start by importing spotify challenge (AI Crowd) file into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON data from file\n",
    "with open('/Users/rishabhhasija/Documents/neuefische/sentify_recommender/data/challenge_set.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize an empty list to collect all track data\n",
    "all_tracks = []\n",
    "\n",
    "# Loop through each playlist in the dataset\n",
    "for playlist in data['playlists']:\n",
    "    for track in playlist['tracks']:\n",
    "        # Add playlist-level information to each track record\n",
    "        track_info = {\n",
    "            'playlist_name': playlist.get('name', 'Unknown'),\n",
    "            'playlist_pid': playlist['pid'],\n",
    "            'playlist_num_tracks': playlist['num_tracks'],\n",
    "            'track_pos': track['pos'],\n",
    "            'artist_name': track['artist_name'],\n",
    "            'track_uri': track['track_uri'],\n",
    "            'artist_uri': track['artist_uri'],\n",
    "            'track_name': track['track_name'],\n",
    "            'album_uri': track['album_uri'],\n",
    "            'duration_ms': track['duration_ms'],\n",
    "            'album_name': track['album_name']\n",
    "        }\n",
    "        all_tracks.append(track_info)\n",
    "\n",
    "# Convert the list of track dictionaries to a DataFrame\n",
    "df_spotify = pd.DataFrame(all_tracks)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "#print(df_spotify.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lyrics Code & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_spotify.shape)\n",
    "#print(df_spotify.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify['playlist_num_tracks'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spotify\n",
    "#df_lyrics = df_spotify['track_uri'].unique()  \n",
    "df_lyrics = df_spotify.drop_duplicates(subset=['track_uri']).drop(['playlist_name', 'playlist_pid', 'playlist_num_tracks', 'track_pos', 'artist_name',\n",
    "       'artist_uri', 'track_name', 'album_uri', 'duration_ms', 'album_name'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df = df_lyrics.head(47397)\n",
    "mini_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics.to_csv('df_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df.to_csv('mini_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import time\n",
    "\n",
    "auth_manager = SpotifyClientCredentials(client_id='59cdbf840d9245118927fc195c3e4e0a', client_secret='f3b0a7237fb04ca2ba9fc6bbefb729b7')\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#def file_exists_and_not_empty(file_name):\n",
    "#    return os.path.isfile(file_name) and os.stat(file_name).st_size > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_data):\n",
    "    # Creating an empty DataFrame to store updated data\n",
    "    processed_data = pd.DataFrame(columns=['track_uri', 'artist_name', 'track_name'])\n",
    "    \n",
    "    # Iterating over the batch_data to fetch details from Spotify\n",
    "    for i, uri in enumerate(batch_data['track_uri']):\n",
    "        try:\n",
    "            track = sp.track(uri)\n",
    "            processed_data.loc[i, 'track_uri'] = uri\n",
    "            processed_data.loc[i, 'artist_name'] = track['artists'][0]['name']\n",
    "            processed_data.loc[i, 'track_name'] = track['name']\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uri}: {e}\")\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "csv_file = \"mini_df.csv\"  # Path to input CSV file\n",
    "output_file = \"mini_test.csv\"  # Path to save the processed data\n",
    "\n",
    "start_row = 46244 # Adjust row number from which processing needs to be started\n",
    "\n",
    "# Read the CSV file in batches and process each batch\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size, skiprows=range(1, start_row)):\n",
    "    time.sleep(5)\n",
    "    processed_chunk = process_batch(chunk)\n",
    "    # Append each processed chunk to the output file\n",
    "    processed_chunk.to_csv(output_file, mode=\"a\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataset & assigning column names\n",
    "df1 = pd.read_csv('mini_test.csv', header=None)\n",
    "df1.columns = ['track_uri', 'artist_name', 'track_name']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1[df1.duplicated(keep=False)]\n",
    "df1_unique = df1.drop_duplicates(keep='first')\n",
    "df1_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ['df1', 'df2']\n",
    "\n",
    "#data2 = [pd.read_csv(f) for f in data1]\n",
    "spotify_data1 = pd.concat([df1, df2], ignore_index=True)\n",
    "spotify_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data = spotify_data1.drop_duplicates(keep='first')\n",
    "#spotify_data.shape\n",
    "spotify_data.to_csv('spotify_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spotify_data = pd.read_csv('/Users/rishabhhasija/Documents/neuefische/sentify_recommender/clean_spotify_data.csv', index_col=None)\n",
    "df = spotify_data.iloc[43999:]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling for Genius API credentials from dotenv file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import lyricsgenius\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"genius_token\")\n",
    "genius = lyricsgenius.Genius(token) # Providing token info to access information \n",
    "genius.remove_section_headers = True # Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.timeout = 60 # Timeout call for API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "import time\n",
    "\n",
    "genius = lyricsgenius.Genius('your_genius_api_token')\n",
    "genius.remove_section_headers = True\n",
    "\n",
    "# Defining the batches at which to save progress\n",
    "batch_size = 10\n",
    "num_songs = len(df)\n",
    "\n",
    "# Specify the starting index from where to resume processing\n",
    "start_index = int(input(\"input value here\"))\n",
    "\n",
    "for i in range(start_index, num_songs, batch_size):\n",
    "    end_index = min(i + batch_size, num_songs)\n",
    "\n",
    "    for j in range(i, end_index):\n",
    "        try:\n",
    "            song = genius.search_song(df.loc[j, 'track_name'], df.loc[j, 'artist_name'])\n",
    "            df.loc[j, 'lyrics'] = song.lyrics.replace('\\n', ' ') if song else 'Lyrics not found'\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching lyrics for index {j}: {e}\")\n",
    "            df.loc[j, 'lyrics'] = 'Error fetching lyrics'\n",
    "\n",
    "    # Save the DataFrame at regular intervals\n",
    "    df.to_csv('lyrics_rishabh.csv', index=False)\n",
    "    print(f\"Progress saved up to index {end_index - 1}\")\n",
    "\n",
    "    #time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lyricsgenius\n",
    "import os\n",
    "\n",
    "# Initialize the Genius API\n",
    "genius = lyricsgenius.Genius('g6Ycf22ZvPBoLt_IARCcp8_DUVzU0EHrGdgOKV_v9yqFutnG2F3HBoR1UJcoFDal')\n",
    "genius.remove_section_headers = True\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'original_data.csv' is your dataset that needs lyrics fetching\n",
    "if os.path.exists('lyrics_rishabh.csv'):\n",
    "    df = pd.read_csv('lyrics_rishabh.csv')\n",
    "else:\n",
    "    df = pd.read_csv('original_data.csv')\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 10\n",
    "num_songs = len(df)\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, num_songs, batch_size):\n",
    "    end_index = min(i + batch_size, num_songs)\n",
    "\n",
    "    for j in range(i, end_index):\n",
    "        # Fetch lyrics only if they are not already present\n",
    "        if pd.isna(df.loc[j, 'lyrics']) or df.loc[j, 'lyrics'] in ['Lyrics not found', 'Error fetching lyrics']:\n",
    "            try:\n",
    "                song = genius.search_song(df.loc[j, 'track_name'], df.loc[j, 'artist_name'])\n",
    "                df.loc[j, 'lyrics'] = song.lyrics.replace('\\n', ' ') if song else 'Lyrics not found'\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching lyrics for index {j}: {e}\")\n",
    "                df.loc[j, 'lyrics'] = 'Error fetching lyrics'\n",
    "\n",
    "    # Save the DataFrame at regular intervals\n",
    "    df.to_csv('lyrics_rishabh.csv', index=False)\n",
    "    print(f\"Progress saved up to index {end_index - 1}\")\n",
    "\n",
    "# Ensure the final save after completion\n",
    "df.to_csv('lyrics_rishabh.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(spotify_data)-1):\n",
    "    song = genius.search_song(spotify_data['track_name'][i], spotify_data['artist_name'][i])\n",
    "    spotify_data.loc[i, 'lyrics'] = song.lyrics if song else 'Lyrics not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch artist name and track name\n",
    "def fetch_tracks_details(track_uris):\n",
    "    try:\n",
    "        tracks_info = sp.tracks(track_uris)\n",
    "        results = []\n",
    "        for track in tracks_info['tracks']:\n",
    "            if track:  # Check if track details are successfully retrieved\n",
    "                artist_name = track['artists'][0]['name'] if track['artists'] else 'Unknown'\n",
    "                track_name = track['name'] if track else 'Unknown'\n",
    "                results.append((artist_name, track_name))\n",
    "            else:\n",
    "                results.append(('Unknown', 'Unknown'))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "        return [('Unknown', 'Unknown')] * len(track_uris)\n",
    "\n",
    "# Apply batch processing\n",
    "batch_size = 50\n",
    "artist_names = []\n",
    "track_names = []\n",
    "\n",
    "for i in range(0, len(df_lyrics['track_uri']), batch_size):\n",
    "    batch_uris = df_lyrics['track_uri'][i:i + batch_size].tolist()\n",
    "    batch_results = fetch_tracks_details(batch_uris)\n",
    "    artist_names.extend([res[0] for res in batch_results])\n",
    "    track_names.extend([res[1] for res in batch_results])\n",
    "\n",
    "df_lyrics['alb_name'] = artist_names\n",
    "df_lyrics['trk_name'] = track_names\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df_lyrics.to_csv('df_lyrics_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting File into multiple files of fixed row size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_per_file = 3000  # Number of rows per file\n",
    "\n",
    "# Calculate how many files will be needed\n",
    "num_files = len(df_lyrics) // rows_per_file + (1 if len(df_lyrics) % rows_per_file != 0 else 0)\n",
    "\n",
    "# Split and save the DataFrame into multiple CSV files\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = start_row + rows_per_file\n",
    "    # Create a new CSV file for each chunk\n",
    "    df_lyrics[start_row:end_row].to_csv(f'output_file_{i + 1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching Lyrics from Genius API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import lyricsgenius\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"genius_token\")\n",
    "genius = lyricsgenius.Genius(token)\n",
    "genius.remove_section_headers = True # Remove section headers from lyrics when searching\n",
    "genius.timeout = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching lyrics via API\n",
    "for i in range(len(df_lyrics)-66233):\n",
    "    df_lyrics.loc[i, 'lyrics'] = genius.search_song(df_lyrics['track_name'][i], df_lyrics['artist_name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_lyrics)-1):\n",
    "    song = genius.search_song(df_lyrics['track_name'][i], df_lyrics['artist_name'][i])\n",
    "    df_lyrics.loc[i, 'lyrics'] = song.lyrics if song else 'Lyrics not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics['lyrics'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"131 ContributorsTranslationsEspañolFrançaisItalianoPortuguêsTeam Lyrics\\nWait till you're announced\\nWe've not yet lost all our graces\\nThe hounds will stay in chains\\nLook upon Your Greatness and she'll\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\nSend the call out, send the call out\\n\\nCall all the ladies out\\nThey're in their finery\\nA hundred jewels on throats\\nA hundred jewels between teeth\\nNow bring my boys in\\nTheir skin in craters like the moon\\nThe moon we love like a brother\\nWhile he glows through the room\\n\\nDancin' around the lies we tell\\nDancin' around big eyes, as well\\nEven the comatose\\nThey don't dance and tell\\nYou might also like\\nWe live in cities you'll never see on-screen\\nNot very pretty, but we sure know how to run things\\nLivin' in ruins of a palace within my dreams\\nAnd you know, we're on each other's team\\n\\nI'm kind of over gettin' told to throw my hands up in the air\\nSo there\\n\\nSo all the cups got broke\\nShards beneath our feet\\nBut it wasn't my fault\\nAnd everyone's competing\\nFor a love they won't receive\\n'Cause what this palace wants is release\\n\\nWe live in cities you'll never see on-screen\\nNot very pretty, but we sure know how to run things\\nLivin' in ruins of a palace within my dreams\\nAnd you know, we're on each other's team\\n\\nI'm kind of over gettin' told to throw my hands up in the air\\nSo there\\nI'm kind of older than I was when I reveled without a care\\nSo there\\nWe live in cities you'll never see on-screen\\nNot very pretty, but we sure know how to run things\\nLivin' in ruins of a palace within my dreams\\nAnd you know, we're on each other's team\\n\\nWe're on each other's team\\nAnd you know, we're on each other's team\\nWe're on each other's team\\nAnd you know, and you know, and you know330Embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics['lyrics'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "# Initialize Genius API\n",
    "genius = lyricsgenius.Genius('put_your_token_here')\n",
    "\n",
    "genius.remove_section_headers = True\n",
    "\n",
    "# Spotify Global Top 50 playlist ID\n",
    "playlist_id = '37i9dQZEVXbMDoHDwVN2tF'  # Example ID, update as necessary\n",
    "\n",
    "results = sp.playlist_tracks(playlist_id)\n",
    "tracks_data = []\n",
    "\n",
    "# Fetch track details from Spotify\n",
    "for item in results['items']:\n",
    "    track = item['track']\n",
    "    track_info = {\n",
    "        'Name': track['name'],\n",
    "        'Artist': track['artists'][0]['name'] if track['artists'] else 'Unknown',\n",
    "        'Album': track['album']['name'] if track['album'] else 'Unknown',\n",
    "        'Release Date': track['album']['release_date'] if track['album'] else 'Unknown',\n",
    "        'Popularity': track['popularity']\n",
    "    }\n",
    "    # Fetch lyrics using Genius\n",
    "    song = genius.search_song(track_info['Name'], track_info['Artist'])\n",
    "    track_info['Lyrics'] = song.lyrics if song else 'Lyrics not found'\n",
    "    \n",
    "    tracks_data.append(track_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lyrics column if it doesn't exist\n",
    "if 'lyrics' not in df_lyrics.columns:\n",
    "    df_lyrics['lyrics'] = None\n",
    "\n",
    "# Defining batch size for progress tracking\n",
    "batch_size = 100\n",
    "num_batches = (len(df_lyrics) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "# Processing each song in df_lyrics\n",
    "for i in range(len(df_lyrics)):\n",
    "    if pd.isna(df_lyrics.at[i, 'lyrics']):  # Check if lyrics already fetched to avoid refetching\n",
    "        try:\n",
    "            song = genius.search_song(df_lyrics.at[i, 'track_name'], df_lyrics.at[i, 'artist_name'])\n",
    "            df_lyrics.at[i, 'lyrics'] = song.lyrics if song else 'Lyrics not found'\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching lyrics for row {i}: {e}\")\n",
    "            df_lyrics.at[i, 'lyrics'] = 'Error fetching lyrics'\n",
    "    # Checking print progress\n",
    "    if i % batch_size == 0 or i == len(df_lyrics) - 1:\n",
    "        print(f\"Processed {i+1}/{len(df_lyrics)} songs, Batch {i // batch_size + 1}/{num_batches}\")\n",
    "\n",
    "# Saving complete DataFrame with lyrics to a new file\n",
    "df_lyrics.to_csv('df_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "df_lyrics.to_csv('spotify_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# Initialize spotipy with Spotify API credentials\n",
    "client_id = '5093ad0ad45f435ca7ce43b2afd406b9'  # Replace with your client ID\n",
    "client_secret = 'f151962f414144aa85494be81ccd8e20'  # Replace with your client secret\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)'''\n",
    "\n",
    "# Function to fetch artist name and track name\n",
    "def fetch_tracks_details(track_uris):\n",
    "    try:\n",
    "        tracks_info = sp.tracks(track_uris)\n",
    "        results = []\n",
    "        for track in tracks_info['tracks']:\n",
    "            if track:  # Check if track details are successfully retrieved\n",
    "                artist_name = track['artists'][0]['name'] if track['artists'] else 'Unknown'\n",
    "                track_name = track['name'] if track else 'Unknown'\n",
    "                results.append((artist_name, track_name))\n",
    "            else:\n",
    "                results.append(('Unknown', 'Unknown'))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "        return [('Unknown', 'Unknown')] * len(track_uris)\n",
    "\n",
    "# Apply batch processing\n",
    "batch_size = 50\n",
    "artist_names = []\n",
    "track_names = []\n",
    "\n",
    "for i in range(0, len(df_lyrics['track_uri']), batch_size):\n",
    "    batch_uris = df_lyrics['track_uri'][i:i + batch_size].tolist()\n",
    "    batch_results = fetch_tracks_details(batch_uris)\n",
    "    artist_names.extend([res[0] for res in batch_results])\n",
    "    track_names.extend([res[1] for res in batch_results])\n",
    "\n",
    "df_lyrics['alb_name'] = artist_names\n",
    "df_lyrics['trk_name'] = track_names\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df_lyrics.to_csv('df_lyrics_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cosine Similarity for Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify_features = df_spotify.drop(columns=['playlist_name', 'playlist_num_tracks', 'artist_name', 'track_name', 'album_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from chunkdot import CosineSimilarityTopK\n",
    "\n",
    "numeric_features = ['playlist_pid', 'track_pos', 'duration_ms']\n",
    "categorical_features = ['track_uri', 'artist_uri', 'album_uri']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cos_sim = CosineSimilarityTopK(top_k=50)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"cos_sim\", cos_sim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pipe.fit_transform(df_spotify_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)\n",
    "# Convert csr.matrix to Dataframe\n",
    "#test_df = pd.DataFrame.sparse.from_spmatrix(test)\n",
    "#df_spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with Track names\n",
    "track_uri = df_spotify['track_uri']\n",
    "indices = pd.Series(df_spotify.index, index=df_spotify['track_uri'])\n",
    "\n",
    "# Function that get Track recommendations based on the cosine similarity \n",
    "def track_recommendations(track, num_recommendations=300):\n",
    "    #get the index of the Track we put into the function\n",
    "    idx = indices.get(track)\n",
    "    #calculate all cosine similarities to that Track and store it in a list\n",
    "    sim_scores = list(enumerate(test[idx].toarray().flatten()))\n",
    "    #sort the list staring with the highest similarity\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    #determine the number of recommendations to retrieve\n",
    "    available_recommendations = min(num_recommendations, len(sim_scores) - 1)\n",
    "    #get the similarities from 1 up to 500 recommendations + 1(not starting with 0 because it is the same Track)\n",
    "    sim_scores = sim_scores[1:available_recommendations + 1]\n",
    "    #get the indices of that 10 Track\n",
    "    track_indices = [i[0] for i in sim_scores]\n",
    "    #retrieve remaining recommendations if not enough unique songs are found\n",
    "    if len(track_indices) < num_recommendations:\n",
    "        all_indices = set(range(test.shape[0]))\n",
    "        used_indices = set(track_indices + [idx])\n",
    "        remaining_indices = list(all_indices - used_indices)\n",
    "\n",
    "        # Fill up the missing slots with any remaining indices\n",
    "        track_indices.extend(remaining_indices[:num_recommendations - len(track_indices)])\n",
    "\n",
    "    #ensuring indices are within bounds\n",
    "    track_indices = [idx for idx in track_indices if idx < len(track_uri)]\n",
    "\n",
    "    #adding extra recommendations if <500\n",
    "    while len(track_indices)<num_recommendations:\n",
    "        track_indices.extend(track_indices[:num_recommendations - len(track_indices)])\n",
    "\n",
    "    unique_track_indices = list(dict.fromkeys(track_indices))\n",
    "\n",
    "    #return the Track names of that 10 Track\n",
    "    return track_uri.iloc[unique_track_indices[:num_recommendations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_recommendations('spotify:track:66U0ASk1VHZsqIkpMjKX3B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from chunkdot import CosineSimilarityTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment intensity analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the compound sentiment score\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']  # Return the compound score\n",
    "\n",
    "# Apply the sentiment analysis function to the cleaned lyrics\n",
    "df['sentiment_score'] = df['lyrics'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sentiment(score):\n",
    "    if score > 0.05:\n",
    "        return 'positive'\n",
    "    elif score < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment label assignment\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(assign_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/sentiment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from file\n",
    "with open('../data/challenge_set.json', 'r') as file: # Replace with local dataset path\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize an empty list to collect all track data\n",
    "all_tracks = []\n",
    "\n",
    "# Loop through each playlist in the dataset\n",
    "for playlist in data['playlists']:\n",
    "    for track in playlist['tracks']:\n",
    "        # Add playlist-level information to each track record\n",
    "        track_info = {\n",
    "            'playlist_name': playlist.get('name', 'Unknown'),\n",
    "            'playlist_pid': playlist['pid'],\n",
    "            'playlist_num_tracks': playlist['num_tracks'],\n",
    "            'track_pos': track['pos'],\n",
    "            'artist_name': track['artist_name'],\n",
    "            'track_uri': track['track_uri'],\n",
    "            'artist_uri': track['artist_uri'],\n",
    "            'track_name': track['track_name'],\n",
    "            'album_uri': track['album_uri'],\n",
    "            'duration_ms': track['duration_ms'],\n",
    "            'album_name': track['album_name']\n",
    "        }\n",
    "        all_tracks.append(track_info)\n",
    "\n",
    "# Convert the list of track dictionaries to a DataFrame\n",
    "df_spotify = pd.DataFrame(all_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify = df_spotify.drop(columns=['playlist_name', 'playlist_num_tracks', 'artist_name', 'track_name', 'album_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('../data/sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify = df_spotify.merge(df_sentiment, on='track_uri', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_pid</th>\n",
       "      <th>track_pos</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>artist_uri</th>\n",
       "      <th>album_uri</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:66U0ASk1VHZsqIkpMjKX3B</td>\n",
       "      <td>spotify:artist:5vCOdeiQt9LyzdI87kt5Sh</td>\n",
       "      <td>spotify:album:4S5MLjwRSi0NJ5nikflYnZ</td>\n",
       "      <td>163809</td>\n",
       "      <td>0.8633</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000</td>\n",
       "      <td>1</td>\n",
       "      <td>spotify:track:5MhsZlmKJG6X5kTHkdwC4B</td>\n",
       "      <td>spotify:artist:5vCOdeiQt9LyzdI87kt5Sh</td>\n",
       "      <td>spotify:album:1qHVYbxQ6IS8YRviorKDJI</td>\n",
       "      <td>166848</td>\n",
       "      <td>0.7938</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000</td>\n",
       "      <td>2</td>\n",
       "      <td>spotify:track:0GZoB8h0kqXn7XFm4Sj06k</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:4UEPxQx0cTcYNsE0n32MHV</td>\n",
       "      <td>232506</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000</td>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:35kahykNu00FPysz3C2euR</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:0rmhjUgoVa17LZuS8xWQ3v</td>\n",
       "      <td>216600</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000</td>\n",
       "      <td>4</td>\n",
       "      <td>spotify:track:3G6hD9B2ZHOsgf4WfNu7X1</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:0rmhjUgoVa17LZuS8xWQ3v</td>\n",
       "      <td>193058</td>\n",
       "      <td>0.8695</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280995</th>\n",
       "      <td>1006767</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:38griAVM808crjbFp9gcPD</td>\n",
       "      <td>spotify:artist:6nnspeopmJAG07xOxHmqTu</td>\n",
       "      <td>spotify:album:2QeEEn8jNy5SFx9coIzS3Z</td>\n",
       "      <td>339573</td>\n",
       "      <td>-0.9905</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280996</th>\n",
       "      <td>1006771</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:1JClFT74TYSXlzpagbmj0S</td>\n",
       "      <td>spotify:artist:1ZwdS5xdxEREPySFridCfh</td>\n",
       "      <td>spotify:album:3PO9OtQdvCDJN8zDLtZiYd</td>\n",
       "      <td>285026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280997</th>\n",
       "      <td>1006773</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:4InLm5a9Qtkru6YxEjM4Qc</td>\n",
       "      <td>spotify:artist:2Y9lO01ABSO8OkBU8FI1mp</td>\n",
       "      <td>spotify:album:5NjFyeZJkYAh5ri9eh8ZSO</td>\n",
       "      <td>279322</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280998</th>\n",
       "      <td>1006775</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:4hdog9vyyqG9pcppG2Izek</td>\n",
       "      <td>spotify:artist:2cFrymmkijnjDg9SS92EPM</td>\n",
       "      <td>spotify:album:1TkwzY3l4LqAfrQwBAx45Q</td>\n",
       "      <td>223295</td>\n",
       "      <td>-0.9581</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280999</th>\n",
       "      <td>1006778</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0NiXXAI876aGImAd6rTj8w</td>\n",
       "      <td>spotify:artist:1HBjj22wzbscIZ9sEb5dyf</td>\n",
       "      <td>spotify:album:0zMLyv1kNV2B0LDGEK2RbB</td>\n",
       "      <td>181614</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        playlist_pid  track_pos                             track_uri   \n",
       "0            1000000          0  spotify:track:66U0ASk1VHZsqIkpMjKX3B  \\\n",
       "1            1000000          1  spotify:track:5MhsZlmKJG6X5kTHkdwC4B   \n",
       "2            1000000          2  spotify:track:0GZoB8h0kqXn7XFm4Sj06k   \n",
       "3            1000000          3  spotify:track:35kahykNu00FPysz3C2euR   \n",
       "4            1000000          4  spotify:track:3G6hD9B2ZHOsgf4WfNu7X1   \n",
       "...              ...        ...                                   ...   \n",
       "280995       1006767          0  spotify:track:38griAVM808crjbFp9gcPD   \n",
       "280996       1006771          0  spotify:track:1JClFT74TYSXlzpagbmj0S   \n",
       "280997       1006773          0  spotify:track:4InLm5a9Qtkru6YxEjM4Qc   \n",
       "280998       1006775          0  spotify:track:4hdog9vyyqG9pcppG2Izek   \n",
       "280999       1006778          0  spotify:track:0NiXXAI876aGImAd6rTj8w   \n",
       "\n",
       "                                   artist_uri   \n",
       "0       spotify:artist:5vCOdeiQt9LyzdI87kt5Sh  \\\n",
       "1       spotify:artist:5vCOdeiQt9LyzdI87kt5Sh   \n",
       "2       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "3       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "4       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "...                                       ...   \n",
       "280995  spotify:artist:6nnspeopmJAG07xOxHmqTu   \n",
       "280996  spotify:artist:1ZwdS5xdxEREPySFridCfh   \n",
       "280997  spotify:artist:2Y9lO01ABSO8OkBU8FI1mp   \n",
       "280998  spotify:artist:2cFrymmkijnjDg9SS92EPM   \n",
       "280999  spotify:artist:1HBjj22wzbscIZ9sEb5dyf   \n",
       "\n",
       "                                   album_uri  duration_ms  sentiment_score   \n",
       "0       spotify:album:4S5MLjwRSi0NJ5nikflYnZ       163809           0.8633  \\\n",
       "1       spotify:album:1qHVYbxQ6IS8YRviorKDJI       166848           0.7938   \n",
       "2       spotify:album:4UEPxQx0cTcYNsE0n32MHV       232506           0.9830   \n",
       "3       spotify:album:0rmhjUgoVa17LZuS8xWQ3v       216600           0.9978   \n",
       "4       spotify:album:0rmhjUgoVa17LZuS8xWQ3v       193058           0.8695   \n",
       "...                                      ...          ...              ...   \n",
       "280995  spotify:album:2QeEEn8jNy5SFx9coIzS3Z       339573          -0.9905   \n",
       "280996  spotify:album:3PO9OtQdvCDJN8zDLtZiYd       285026              NaN   \n",
       "280997  spotify:album:5NjFyeZJkYAh5ri9eh8ZSO       279322           0.9956   \n",
       "280998  spotify:album:1TkwzY3l4LqAfrQwBAx45Q       223295          -0.9581   \n",
       "280999  spotify:album:0zMLyv1kNV2B0LDGEK2RbB       181614           0.9932   \n",
       "\n",
       "       sentiment_label  \n",
       "0             positive  \n",
       "1             positive  \n",
       "2             positive  \n",
       "3             positive  \n",
       "4             positive  \n",
       "...                ...  \n",
       "280995        negative  \n",
       "280996             NaN  \n",
       "280997        positive  \n",
       "280998        negative  \n",
       "280999        positive  \n",
       "\n",
       "[281000 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_pid</th>\n",
       "      <th>track_pos</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>artist_uri</th>\n",
       "      <th>album_uri</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [playlist_pid, track_pos, track_uri, artist_uri, album_uri, duration_ms, sentiment_score, sentiment_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify[df_spotify['sentiment_score'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258678, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spotify.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_pid</th>\n",
       "      <th>track_pos</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>artist_uri</th>\n",
       "      <th>album_uri</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:66U0ASk1VHZsqIkpMjKX3B</td>\n",
       "      <td>spotify:artist:5vCOdeiQt9LyzdI87kt5Sh</td>\n",
       "      <td>spotify:album:4S5MLjwRSi0NJ5nikflYnZ</td>\n",
       "      <td>163809</td>\n",
       "      <td>0.8633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000</td>\n",
       "      <td>1</td>\n",
       "      <td>spotify:track:5MhsZlmKJG6X5kTHkdwC4B</td>\n",
       "      <td>spotify:artist:5vCOdeiQt9LyzdI87kt5Sh</td>\n",
       "      <td>spotify:album:1qHVYbxQ6IS8YRviorKDJI</td>\n",
       "      <td>166848</td>\n",
       "      <td>0.7938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000</td>\n",
       "      <td>2</td>\n",
       "      <td>spotify:track:0GZoB8h0kqXn7XFm4Sj06k</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:4UEPxQx0cTcYNsE0n32MHV</td>\n",
       "      <td>232506</td>\n",
       "      <td>0.9830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000</td>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:35kahykNu00FPysz3C2euR</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:0rmhjUgoVa17LZuS8xWQ3v</td>\n",
       "      <td>216600</td>\n",
       "      <td>0.9978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000</td>\n",
       "      <td>4</td>\n",
       "      <td>spotify:track:3G6hD9B2ZHOsgf4WfNu7X1</td>\n",
       "      <td>spotify:artist:163tK9Wjr9P9DmM0AVK7lm</td>\n",
       "      <td>spotify:album:0rmhjUgoVa17LZuS8xWQ3v</td>\n",
       "      <td>193058</td>\n",
       "      <td>0.8695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280994</th>\n",
       "      <td>1006752</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:6FI3RJ58Ztl0X1VtA6pVs9</td>\n",
       "      <td>spotify:artist:09hVIj6vWgoCDtT03h8ZCa</td>\n",
       "      <td>spotify:album:4v5x3Oo3UjQ9YmF3hRAip5</td>\n",
       "      <td>208840</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280995</th>\n",
       "      <td>1006767</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:38griAVM808crjbFp9gcPD</td>\n",
       "      <td>spotify:artist:6nnspeopmJAG07xOxHmqTu</td>\n",
       "      <td>spotify:album:2QeEEn8jNy5SFx9coIzS3Z</td>\n",
       "      <td>339573</td>\n",
       "      <td>-0.9905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280997</th>\n",
       "      <td>1006773</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:4InLm5a9Qtkru6YxEjM4Qc</td>\n",
       "      <td>spotify:artist:2Y9lO01ABSO8OkBU8FI1mp</td>\n",
       "      <td>spotify:album:5NjFyeZJkYAh5ri9eh8ZSO</td>\n",
       "      <td>279322</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280998</th>\n",
       "      <td>1006775</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:4hdog9vyyqG9pcppG2Izek</td>\n",
       "      <td>spotify:artist:2cFrymmkijnjDg9SS92EPM</td>\n",
       "      <td>spotify:album:1TkwzY3l4LqAfrQwBAx45Q</td>\n",
       "      <td>223295</td>\n",
       "      <td>-0.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280999</th>\n",
       "      <td>1006778</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:0NiXXAI876aGImAd6rTj8w</td>\n",
       "      <td>spotify:artist:1HBjj22wzbscIZ9sEb5dyf</td>\n",
       "      <td>spotify:album:0zMLyv1kNV2B0LDGEK2RbB</td>\n",
       "      <td>181614</td>\n",
       "      <td>0.9932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258678 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        playlist_pid  track_pos                             track_uri   \n",
       "0            1000000          0  spotify:track:66U0ASk1VHZsqIkpMjKX3B  \\\n",
       "1            1000000          1  spotify:track:5MhsZlmKJG6X5kTHkdwC4B   \n",
       "2            1000000          2  spotify:track:0GZoB8h0kqXn7XFm4Sj06k   \n",
       "3            1000000          3  spotify:track:35kahykNu00FPysz3C2euR   \n",
       "4            1000000          4  spotify:track:3G6hD9B2ZHOsgf4WfNu7X1   \n",
       "...              ...        ...                                   ...   \n",
       "280994       1006752          0  spotify:track:6FI3RJ58Ztl0X1VtA6pVs9   \n",
       "280995       1006767          0  spotify:track:38griAVM808crjbFp9gcPD   \n",
       "280997       1006773          0  spotify:track:4InLm5a9Qtkru6YxEjM4Qc   \n",
       "280998       1006775          0  spotify:track:4hdog9vyyqG9pcppG2Izek   \n",
       "280999       1006778          0  spotify:track:0NiXXAI876aGImAd6rTj8w   \n",
       "\n",
       "                                   artist_uri   \n",
       "0       spotify:artist:5vCOdeiQt9LyzdI87kt5Sh  \\\n",
       "1       spotify:artist:5vCOdeiQt9LyzdI87kt5Sh   \n",
       "2       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "3       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "4       spotify:artist:163tK9Wjr9P9DmM0AVK7lm   \n",
       "...                                       ...   \n",
       "280994  spotify:artist:09hVIj6vWgoCDtT03h8ZCa   \n",
       "280995  spotify:artist:6nnspeopmJAG07xOxHmqTu   \n",
       "280997  spotify:artist:2Y9lO01ABSO8OkBU8FI1mp   \n",
       "280998  spotify:artist:2cFrymmkijnjDg9SS92EPM   \n",
       "280999  spotify:artist:1HBjj22wzbscIZ9sEb5dyf   \n",
       "\n",
       "                                   album_uri  duration_ms  sentiment_score  \n",
       "0       spotify:album:4S5MLjwRSi0NJ5nikflYnZ       163809           0.8633  \n",
       "1       spotify:album:1qHVYbxQ6IS8YRviorKDJI       166848           0.7938  \n",
       "2       spotify:album:4UEPxQx0cTcYNsE0n32MHV       232506           0.9830  \n",
       "3       spotify:album:0rmhjUgoVa17LZuS8xWQ3v       216600           0.9978  \n",
       "4       spotify:album:0rmhjUgoVa17LZuS8xWQ3v       193058           0.8695  \n",
       "...                                      ...          ...              ...  \n",
       "280994  spotify:album:4v5x3Oo3UjQ9YmF3hRAip5       208840           0.9996  \n",
       "280995  spotify:album:2QeEEn8jNy5SFx9coIzS3Z       339573          -0.9905  \n",
       "280997  spotify:album:5NjFyeZJkYAh5ri9eh8ZSO       279322           0.9956  \n",
       "280998  spotify:album:1TkwzY3l4LqAfrQwBAx45Q       223295          -0.9581  \n",
       "280999  spotify:album:0zMLyv1kNV2B0LDGEK2RbB       181614           0.9932  \n",
       "\n",
       "[258678 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the Label, keep sentiment score\n",
    "df_spotify.drop(columns=['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['playlist_pid', 'track_pos', 'sentiment_score']\n",
    "categorical_features = ['track_uri', 'artist_uri', 'album_uri']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cos_sim = CosineSimilarityTopK(top_k=50)\n",
    "\n",
    "cos_sim_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"cos_sim\", cos_sim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;playlist_pid&#x27;, &#x27;track_pos&#x27;,\n",
       "                                                   &#x27;sentiment_score&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [&#x27;track_uri&#x27;, &#x27;artist_uri&#x27;,\n",
       "                                                   &#x27;album_uri&#x27;])])),\n",
       "                (&#x27;cos_sim&#x27;, CosineSimilarityTopK(top_k=50))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;playlist_pid&#x27;, &#x27;track_pos&#x27;,\n",
       "                                                   &#x27;sentiment_score&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [&#x27;track_uri&#x27;, &#x27;artist_uri&#x27;,\n",
       "                                                   &#x27;album_uri&#x27;])])),\n",
       "                (&#x27;cos_sim&#x27;, CosineSimilarityTopK(top_k=50))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;playlist_pid&#x27;, &#x27;track_pos&#x27;,\n",
       "                                  &#x27;sentiment_score&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;, OneHotEncoder())]),\n",
       "                                 [&#x27;track_uri&#x27;, &#x27;artist_uri&#x27;, &#x27;album_uri&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;playlist_pid&#x27;, &#x27;track_pos&#x27;, &#x27;sentiment_score&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;track_uri&#x27;, &#x27;artist_uri&#x27;, &#x27;album_uri&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CosineSimilarityTopK</label><div class=\"sk-toggleable__content\"><pre>CosineSimilarityTopK(top_k=50)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['playlist_pid', 'track_pos',\n",
       "                                                   'sentiment_score']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['track_uri', 'artist_uri',\n",
       "                                                   'album_uri'])])),\n",
       "                ('cos_sim', CosineSimilarityTopK(top_k=50))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cos_sim_pipeline.fit_transform(df_spotify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csr.matrix to Dataframe\n",
    "sim_matrix_df = pd.DataFrame.sparse.from_spmatrix(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with track uris\n",
    "track_uri = df_spotify['track_uri']\n",
    "indices = pd.Series(df_spotify.index, index=df_spotify['track_uri'])\n",
    "\n",
    "# Function that get track recommendations based on the cosine similarity \n",
    "def track_recommendations(track):\n",
    "\n",
    "    #get the index of the track we put into the function\n",
    "    idx = indices[track].iloc[0]\n",
    "\n",
    "    \"\"\"#catch duplicates\n",
    "    # duplicates = indices[track].iloc[1:]\n",
    "    # dup_list = list(enumerate(sim_matrix_df[duplicates]))\"\"\"\n",
    "\n",
    "    #calculate all cosine similarities to that track and store it in a list\n",
    "    sim_scores = list(enumerate(sim_matrix_df[idx]))\n",
    "\n",
    "    \"\"\"#remove duplicates from recommendation list\n",
    "    remove = []\n",
    "    for tup in sim_scores:\n",
    "        for index in dup_list:\n",
    "            if tup[0] == index[1]:\n",
    "                remove.append(tup) \n",
    "    print(sorted(remove, key=lambda x: x[1], reverse=True)) #Test\n",
    "\n",
    "    sim_scores = [x for x in sim_scores if x not in remove] \"\"\"\n",
    "\n",
    "    #sort the list staring with the highest similarity\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    #sim_scores = sim_scores.drop_duplicates(keep='first')\n",
    "\n",
    "    \"\"\"# get the similarities from 1:1001 (not starting with 0 because it is the same track)\n",
    "    # We overshoot here on purpose so there is leeway to remove duplicates and still end up \"\"\"\n",
    "    # with the correct amount of predictions to return (this is a very lazy fix...)\n",
    "    sim_scores = sim_scores[1:100]\n",
    "\n",
    "    #get the indeces of that 1000 tracks\n",
    "    track_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Remove duplicates from our selection of 1000 tracks and\n",
    "    # return the track uris of a duplicate-free subset of 500 tracks\n",
    "    recommended_tracks = track_uri.iloc[track_indices].drop_duplicates(keep='first').iloc[1:4]\n",
    "    return recommended_tracks.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_list = ['spotify:track:0uppYCG86ajpV2hSR3dJJ0', 'spotify:track:5MhsZlmKJG6X5kTHkdwC4B']\n",
    "recommended = []\n",
    "for track in track_list:\n",
    "    #print(track)\n",
    "    x = track_recommendations(track)\n",
    "    for i in x:\n",
    "        recommended.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the preprocessor pipeline\n",
    "joblib.dump(preprocessor, 'preprocessor_pipeline.pkl')\n",
    "\n",
    "# Save the similarity matrix\n",
    "joblib.dump(sim_matrix_df, 'similarity_matrix.pkl')\n",
    "\n",
    "# Save the track_uri and indices\n",
    "df_spotify[['track_uri']].to_pickle('track_uri.pkl')\n",
    "indices.to_pickle('indices.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255      spotify:track:2J1t2b8xPBqZzj5znx0C7C\n",
      "31278    spotify:track:4YmJGDRo6oGhObb7MaW5om\n",
      "1300     spotify:track:4JLojdKkKNM3rgvP2zvUGR\n",
      "Name: track_uri, dtype: object]\n",
      "[255      spotify:track:2J1t2b8xPBqZzj5znx0C7C\n",
      "31278    spotify:track:4YmJGDRo6oGhObb7MaW5om\n",
      "1300     spotify:track:4JLojdKkKNM3rgvP2zvUGR\n",
      "Name: track_uri, dtype: object, 16702    spotify:track:5MhsZlmKJG6X5kTHkdwC4B\n",
      "Name: track_uri, dtype: object]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m recommended \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m track_list:\n\u001b[0;32m----> 4\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrack_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(recommended)\n",
      "Cell \u001b[0;32mIn[153], line 10\u001b[0m, in \u001b[0;36mtrack_recommendations\u001b[0;34m(track, top_n)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_recommendations\u001b[39m(track, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#get the index of the track we put into the function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#catch duplicates\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# duplicates = indices[track].iloc[1:]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# dup_list = list(enumerate(sim_matrix_df[duplicates]))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#calculate all cosine similarities to that track and store it in a list\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     sim_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(sim_matrix_df[idx]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "track_list = ['spotify:track:2UYJqglnOMTvRcqQLNcjjf', 'spotify:track:5MhsZlmKJG6X5kTHkdwC4B', 'spotify:track:38griAVM808crjbFp9gcPD']\n",
    "recommended = []\n",
    "for track in track_list:\n",
    "    recommended.append(track_recommendations(track, 3))\n",
    "    print(recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = track_recommendations('spotify:track:2UYJqglnOMTvRcqQLNcjjf', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_list = d1.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrect Track Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_recommendations(tracks, top_n=10):\n",
    "    # Initialize an empty list to store the track indices\n",
    "    track_indices = []\n",
    "\n",
    "    # Iterate over each track in the list\n",
    "    for track in tracks:\n",
    "        # Get the index of the track\n",
    "        idx = indices.loc[track]\n",
    "\n",
    "        # Check if the track index is present in the similarity matrix\n",
    "        if idx in sim_matrix_df.index:\n",
    "            # Calculate the cosine similarity scores for the track\n",
    "            sim_scores = list(enumerate(sim_matrix_df.loc[idx]))\n",
    "\n",
    "            # Sort the scores in descending order\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Add the track indices to the list\n",
    "            track_indices.extend([i[0] for i in sim_scores[1:top_n+1]])\n",
    "\n",
    "    # Calculate the average cosine similarity for each track index\n",
    "    avg_sim_scores = {}\n",
    "    for idx in track_indices:\n",
    "        avg_sim_scores[idx] = avg_sim_scores.get(idx, 0) + 1\n",
    "\n",
    "    # Sort the track indices by their average similarity score\n",
    "    sorted_indices = sorted(avg_sim_scores, key=avg_sim_scores.get, reverse=True)\n",
    "\n",
    "    # Get the top N recommended tracks\n",
    "    recommended_tracks = track_uri.iloc[sorted_indices[:top_n]]\n",
    "\n",
    "    return recommended_tracks.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recommend \u001b[38;5;241m=\u001b[39m \u001b[43mtrack_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspotify:track:2UYJqglnOMTvRcqQLNcjjf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspotify:track:5MhsZlmKJG6X5kTHkdwC4B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspotify:track:38griAVM808crjbFp9gcPD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommend)\n",
      "Cell \u001b[0;32mIn[145], line 11\u001b[0m, in \u001b[0;36mtrack_recommendations\u001b[0;34m(tracks, top_n)\u001b[0m\n\u001b[1;32m      8\u001b[0m idx \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mloc[track]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if the track index is present in the similarity matrix\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msim_matrix_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Calculate the cosine similarity scores for the track\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     sim_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(sim_matrix_df\u001b[38;5;241m.\u001b[39mloc[idx]))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Sort the scores in descending order\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexes/range.py:326\u001b[0m, in \u001b[0;36mRangeIndex.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28mhash\u001b[39m(key)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m         key \u001b[38;5;241m=\u001b[39m ensure_python_int(key)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "recommend = track_recommendations(['spotify:track:2UYJqglnOMTvRcqQLNcjjf', 'spotify:track:5MhsZlmKJG6X5kTHkdwC4B', 'spotify:track:38griAVM808crjbFp9gcPD'])\n",
    "print(recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to get recommendations for 3 songs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def track_recommendations(track_uris, top_n=10):\n",
    "    # Ensure input is in list format even if a single track URI is passed\n",
    "    if isinstance(track_uris, str):\n",
    "        track_uris = [track_uris]\n",
    "\n",
    "    # Initialize sum of similarity scores\n",
    "    sim_scores_sum = None\n",
    "\n",
    "    # Loop through each track URI provided\n",
    "    for track in track_uris:\n",
    "        idx = indices[track]  # Directly get the index of the track\n",
    "        if sim_scores_sum is None:\n",
    "            sim_scores_sum = sim_matrix_df.iloc[idx].to_numpy(copy=True)  # Start with the first track's similarity scores\n",
    "        else:\n",
    "            sim_scores_sum += sim_matrix_df.iloc[idx].to_numpy()  # Add the similarity scores of subsequent tracks\n",
    "\n",
    "    # Convert the accumulated numpy array back to pandas Series to use pandas' nlargest function\n",
    "    avg_sim_scores = pd.Series(sim_scores_sum / len(track_uris), index=sim_matrix_df.columns)\n",
    "\n",
    "    # Get the indices of the top N similar tracks\n",
    "    top_indices = avg_sim_scores.nlargest(top_n + 1).index  # Get top N+1 indices to avoid self-recommendation\n",
    "\n",
    "    # Get the recommended tracks, skipping any that are in the input list\n",
    "    recommended_tracks = track_uri.iloc[top_indices].drop_duplicates().iloc[1:top_n+1]  # Skip the highest if it's an input track\n",
    "\n",
    "    return recommended_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1618\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3946\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3947\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3948\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3949\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 3932\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:960\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_indices:\n\u001b[0;32m--> 960\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexers/utils.py:284\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m track_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspotify:track:2UYJqglnOMTvRcqQLNcjjf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspotify:track:5MhsZlmKJG6X5kTHkdwC4B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspotify:track:38griAVM808crjbFp9gcPD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m recommended \u001b[38;5;241m=\u001b[39m \u001b[43mtrack_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommended)\n",
      "Cell \u001b[0;32mIn[147], line 17\u001b[0m, in \u001b[0;36mtrack_recommendations\u001b[0;34m(track_uris, top_n)\u001b[0m\n\u001b[1;32m     15\u001b[0m idx \u001b[38;5;241m=\u001b[39m indices[track]  \u001b[38;5;66;03m# Directly get the index of the track\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sim_scores_sum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     sim_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[43msim_matrix_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Start with the first track's similarity scores\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     sim_scores_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sim_matrix_df\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mto_numpy()  \u001b[38;5;66;03m# Add the similarity scores of subsequent tracks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1651\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1621\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "track_list = ['spotify:track:2UYJqglnOMTvRcqQLNcjjf', 'spotify:track:5MhsZlmKJG6X5kTHkdwC4B', 'spotify:track:38griAVM808crjbFp9gcPD']\n",
    "recommended = track_recommendations(track_list, 10)\n",
    "print(recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorde_list = lorde.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lorde = lorde.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lorde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_lorde[df_lorde['track_uri'] == 'spotify:track:7yyRTcZmCiyzzJlNzGC9Ol']\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lorde[df_lorde['track_uri'] == 'spotify:track:7yyRTcZmCiyzzJlNzGC9Ol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Playlist on Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class CreatePlaylist:\n",
    "    \"\"\"\n",
    "    A class for accessing your spotify account via the API using spotipy.\n",
    "    Make sure to have a .env file prepared with your client id and client secret.\n",
    "    Above you find an example use for using this class and the methods.\n",
    "\n",
    "    Example usage:\n",
    "    spotify_api = CreatePlaylist()\n",
    "    my_playlist = spotify_api.create_playlist(name=\"TestAutomaticPlaylistGeneration\", description=\"Automatically generated playlist\")\n",
    "    test_uri = ['spotify:track:7yyRTcZmCiyzzJlNzGC9Ol']\n",
    "    spotify_api.add_tracks_to_playlist(my_playlist['id'], test_uri)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.sp = spotipy.Spotify(auth_manager=SpotifyOAuth(\n",
    "            client_id=os.getenv(\"spotify_client_id\"),\n",
    "            client_secret=os.getenv(\"spotify_client_secret\"),\n",
    "            redirect_uri=\"http://localhost:8888/callback\",\n",
    "            scope='user-library-read playlist-modify-private',\n",
    "            cache_path=\"token.txt\"))\n",
    "\n",
    "    def create_playlist(self, name, description, public=False):\n",
    "        \"\"\" \n",
    "        A method from spotipy that creates an empty Spotify playlist for the account signed in via the API.\n",
    "        It takes the playlist name and description and additional features (public and user_id)\n",
    "        \"\"\"\n",
    "        results = self.sp.current_user()\n",
    "        user_id = results['id']\n",
    "        my_playlist = self.sp.user_playlist_create(user=f\"{user_id}\", name=name, public=public, description=description)\n",
    "        return my_playlist\n",
    "        \n",
    "\n",
    "    def add_tracks_to_playlist(self, playlist_id, track_uris, position=None):\n",
    "        \"\"\" \n",
    "        A method from spotipy that adds track to a Spotify playlist.\n",
    "        Takes the playlist_id and a list of track URIs as arguments (and optionally the position of a track)\n",
    "        \"\"\"\n",
    "        self.sp.playlist_add_items(playlist_id=playlist_id, items=track_uris, position=position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhhasija/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/spotipy/oauth2.py:338: DeprecationWarning: Specifying cache_path or username as arguments to SpotifyOAuth will be deprecated. Instead, please create a CacheFileHandler instance with the desired cache_path and username and pass it to SpotifyOAuth as the cache_handler. For example:\n",
      "\n",
      "\tfrom spotipy.oauth2 import CacheFileHandler\n",
      "\thandler = CacheFileHandler(cache_path=cache_path, username=username)\n",
      "\tsp = spotipy.SpotifyOAuth(client_id, client_secret, redirect_uri, cache_handler=handler)\n",
      "  warnings.warn(\"Specifying cache_path or username as arguments to SpotifyOAuth \" +\n"
     ]
    }
   ],
   "source": [
    "spotify_api = CreatePlaylist()\n",
    "my_playlist = spotify_api.create_playlist(name=\"TestAutomaticPlaylistGeneration\", description=\"Automatically generated playlist\")\n",
    "test_uri = d1_list\n",
    "spotify_api.add_tracks_to_playlist(my_playlist['id'], test_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics - Not to be Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Precision & Recall for Evaluation\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    # Convert predicted to set and limit to top k items\n",
    "    pred_set = set(predicted[:k])\n",
    "    # Convert actual to set\n",
    "    actual_set = set(actual)\n",
    "    # Calculate intersection of predicted and actual\n",
    "    relevant_and_recommended = pred_set.intersection(actual_set)\n",
    "    # Return the proportion of relevant items in the top k recommendations\n",
    "    return len(relevant_and_recommended) / float(k)\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    # Convert predicted to set and limit to top k items\n",
    "    pred_set = set(predicted[:k])\n",
    "    # Convert actual to set\n",
    "    actual_set = set(actual)\n",
    "    # Calculate intersection of predicted and actual\n",
    "    relevant_and_recommended = pred_set.intersection(actual_set)\n",
    "    # Return the proportion of relevant items that are actually recommended\n",
    "    return len(relevant_and_recommended) / float(len(actual_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify playlists that contain test 'track_uri'\n",
    "playlists_with_track = df_spotify[df_spotify['track_uri'] == 'spotify:track:0GZoB8h0kqXn7XFm4Sj06k']['playlist_pid'].unique()\n",
    "\n",
    "# Collecting all tracks from these playlists\n",
    "actual_tracks = df_spotify[df_spotify['playlist_pid'].isin(playlists_with_track)]['track_uri'].unique().tolist()\n",
    "\n",
    "# Ensure that 'some_track_uri' is not in the actual tracks to avoid self-recommendation\n",
    "actual_tracks.remove('spotify:track:0GZoB8h0kqXn7XFm4Sj06k')\n",
    "\n",
    "recommended_tracks = track_recommendations('spotify:track:0GZoB8h0kqXn7XFm4Sj06k', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6\n",
      "Recall: 0.007255139056831923\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall at k=10\n",
    "precision = precision_at_k(actual_tracks, recommended_tracks, 10)\n",
    "recall = recall_at_k(actual_tracks, recommended_tracks, 10)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model creation using Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input DataFrame `df` containing the frequent itemsets is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m frequent_itemsets \u001b[38;5;241m=\u001b[39m apriori(df, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, use_colnames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Generate association rules from the frequent item sets with a confidence threshold of 70%\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m rules \u001b[38;5;241m=\u001b[39m \u001b[43massociation_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrequent_itemsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Print the resulting rules with their antecedents, consequents, support, and confidence\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(rules[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantecedents\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsequents\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[0;32m~/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/mlxtend/frequent_patterns/association_rules.py:83\u001b[0m, in \u001b[0;36massociation_rules\u001b[0;34m(df, metric, min_threshold, support_only)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates a DataFrame of association rules including the\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mmetrics 'score', 'confidence', and 'lift'\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input DataFrame `df` containing \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe frequent itemsets is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# check for mandatory columns\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitemsets\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "\u001b[0;31mValueError\u001b[0m: The input DataFrame `df` containing the frequent itemsets is empty."
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Gathering all tracks for each playlist, resulting in a list of playlists, with each playlist a list of tracks\n",
    "playlists = df_spotify.groupby('playlist_pid')['track_uri'].apply(list).tolist()\n",
    "\n",
    "# Initialize the transaction encoder to transform lists of tracks into a one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(playlists).transform(playlists)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)  # DataFrame to hold the binary attributes\n",
    "\n",
    "# Generate frequent item sets that have a support of at least 50%\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent item sets with a confidence threshold of 70%\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.001)\n",
    "\n",
    "# Print the resulting rules with their antecedents, consequents, support, and confidence\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Result: FAILED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_spotify is your DataFrame and it includes 'duration_ms' and 'sentiment_score'\n",
    "features = df_spotify[['playlist_pid', 'sentiment_score']]#, 'track_uri', 'artist_uri', 'album_uri']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Initialize KMeans with a guessed number of clusters (e.g., 5)\n",
    "kmeans = KMeans(n_clusters=20, random_state=0)\n",
    "df_spotify['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Check the count of tracks in each cluster\n",
    "cluster_counts = df_spotify['cluster'].value_counts()\n",
    "print(cluster_counts)\n",
    "\n",
    "# Optional: Analyze characteristics of each cluster\n",
    "for i in range(5):\n",
    "    cluster_mean = df_spotify[df_spotify['cluster'] == i][['playlist_pid', 'sentiment_score']]#, 'track_uri', 'artist_uri', 'album_uri']].mean()\n",
    "    print(f\"Cluster {i} mean duration and sentiment score:\", cluster_mean);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Creating a single feature for track, artist, album uri's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhhasija/Documents/neuefische/sentify_recommender/.venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playlist_cluster\n",
      "0    258488\n",
      "4        98\n",
      "2        90\n",
      "3         1\n",
      "1         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combining track, artist, and album URIs into a single 'feature' for each playlist\n",
    "df_spotify['combined'] = df_spotify['track_uri'] + ' ' + df_spotify['artist_uri'] + ' ' + df_spotify['album_uri']\n",
    "\n",
    "# Create matrix of combined\n",
    "vectorizer = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "playlist_features = vectorizer.fit_transform(df_spotify.groupby('playlist_pid')['combined'].apply(' '.join))\n",
    "\n",
    "# Adding the mean of sentiment score as a feature\n",
    "sentiment_scores = df_spotify.groupby('playlist_pid')['sentiment_score'].mean().values\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "playlist_features_scaled = scaler.fit_transform(playlist_features.toarray())\n",
    "\n",
    "# Adding sentiment score as a feature\n",
    "import numpy as np\n",
    "playlist_features_final = np.hstack([playlist_features_scaled, sentiment_scores.reshape(-1, 1)])\n",
    "\n",
    "# Cluster playlists\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "playlist_clusters = kmeans.fit_predict(playlist_features_final)\n",
    "\n",
    "# Map clusters back to original data\n",
    "df_spotify['playlist_cluster'] = df_spotify['playlist_pid'].map(dict(zip(df_spotify['playlist_pid'].unique(), playlist_clusters)))\n",
    "\n",
    "# Check the distribution of playlists across clusters\n",
    "print(df_spotify['playlist_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with track identifiers\n",
    "track_uri = df_spotify['track_uri']\n",
    "indices = pd.Series(df_spotify.index, index=df_spotify['track_uri'])\n",
    "\n",
    "# Function that get track recommendations based on the cosine similarity \n",
    "def track_recommendations(track, top_n=200):\n",
    "\n",
    "    #get the index of the track we put into the function\n",
    "    idx = indices[track].iloc[0]\n",
    "\n",
    "    #catch duplicates\n",
    "    # duplicates = indices[track].iloc[1:]\n",
    "    # dup_list = list(enumerate(sim_matrix_df[duplicates]))\n",
    "\n",
    "    #calculate all cosine similarities to that track and store it in a list\n",
    "    sim_scores = list(enumerate(sim_matrix_df[idx]))\n",
    "\n",
    "    #remove duplicates from recommendation list\n",
    "    \"\"\" remove = []\n",
    "    for tup in sim_scores:\n",
    "        for index in dup_list:\n",
    "            if tup[0] == index[1]:\n",
    "                remove.append(tup) \n",
    "    print(sorted(remove, key=lambda x: x[1], reverse=True)) #Test\n",
    "\n",
    "    sim_scores = [x for x in sim_scores if x not in remove] \"\"\"\n",
    "\n",
    "    #sort the list staring with the highest similarity\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    #sim_scores = sim_scores.drop_duplicates(keep='first')\n",
    "\n",
    "    # get the similarities from 1:1001 (not starting with 0 because it is the same track)\n",
    "    # We overshoot here on purpose so there is leeway to remove duplicates and still end up \n",
    "    # with the correct amount of predictions to return (this is a very lazy fix...)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "\n",
    "    #get the indices of that 1000 tracks\n",
    "    track_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Remove duplicates from our selection of 1000 tracks and\n",
    "    # return the track uris of a duplicate-free subset of 500 tracks\n",
    "    recommended_tracks = track_uri.iloc[track_indices].drop_duplicates(keep='first').iloc[:top_n]\n",
    "    return recommended_tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
